{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Installs, Imports and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install spacy==3.1.1 #restart runtime after this\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import string\n",
    "import operator\n",
    "from itertools import islice\n",
    "from collections import Counter\n",
    "\n",
    "#!pip install nltk\n",
    "from nltk import ngrams\n",
    "import nltk as nltk\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet');\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  2. Reading files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Data: 666\n",
      "\n",
      "Columns :  ['Filename', 'Text', 'Type', 'Year']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/alertas.csv', sep=\"|\")\n",
    "df = df[df[\"Text\"] != \" \"]\n",
    "print(\"Size of Data:\", len(df))\n",
    "print()\n",
    "print(\"Columns : \", list(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 3920760000\n",
    "stop_words = stopwords.words('english') \n",
    "\n",
    "punct_signs = list(string.punctuation)\n",
    "punct_signs += ['‚Ä¶','¬ø','‚Ä¢','‚Äù','‚Äú','‚Äì','&','‚àë','[',']', '‚ñ™', \"I ‚Äôm\", \"Ô∏è‚ù§\", \"‚†Ä ‚†Ä ‚†Ä\", \"‚†Ä ‚†Ä\"]\n",
    "\n",
    "sim_dict = {\n",
    "    \"'\" : ['‚Äô'],\n",
    "   # \"Ô∏è‚ù§\": [\"Ô∏è ‚ù§ Ô∏è\", \"‚ù§ Ô∏è\", \"Ô∏è ‚ù§\"]\n",
    "    \n",
    "}\n",
    "\n",
    "emojies = [\"ü§ö\",\"üôã\",\"üò≥\",\"üèª‚Äç\", \"‚ôÄ\", \"üèª‚Äç\", \"üìà\", \"Ô∏è ‚¨Ü\", \"‚ù§\", \"üèº\", \"üíú\", \"üëè\"]\n",
    "\n",
    "def replace_simliars(text, sim_dict):\n",
    "    for key in sim_dict.keys():\n",
    "        for v in sim_dict[key]:\n",
    "            text = text.replace(v, key)\n",
    "    return text\n",
    "\n",
    "#clean stopwords and punctuation, lemmatization\n",
    "def clean_text(text):   \n",
    "    clean_text = []\n",
    "    text = replace_simliars(text, sim_dict)\n",
    "    for p in punct_signs:\n",
    "        text = text.replace(p, ' ') \n",
    "        \n",
    "    #for e in emojies:\n",
    "     #   text = text.replace(e, ' ') \n",
    "    \n",
    "    clean_text = text.lower().split()\n",
    "    clean_text = [w for w in clean_text if w not in stop_words]\n",
    "    \n",
    "    nlp.max_length = len(' '.join(clean_text)) + 100\n",
    "    doc = nlp(' '.join(clean_text))\n",
    "    txt = []\n",
    "    for token in doc:  \n",
    "        if token.lemma_ not in stop_words and token.lemma:\n",
    "            txt.append(token.lemma_)\n",
    "    return txt\n",
    "\n",
    "#m is the number of top ngrams.\n",
    "def getNGrams(text, n, m):\n",
    "    h_dict = {}   \n",
    "    ngramas = list(ngrams(text, n))\n",
    "    for grams in ngramas:\n",
    "        words = ' '.join(grams).strip()\n",
    "        if words not in h_dict:\n",
    "            h_dict[words] = 0\n",
    "        h_dict[words] = h_dict[words] + 1\n",
    "        \n",
    "    sorted_dict = sorted(h_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    top_values = list(islice(sorted_dict, m))\n",
    "    return [(r[0], r[1], round(r[1]/len(ngramas)*100,2)) for r in top_values]\n",
    "\n",
    "def printNgrams(data, m):\n",
    "    Ngrams = []\n",
    "    unigrams = getNGrams(data, 1, m)\n",
    "    bigrams = getNGrams(data, 2 , m)\n",
    "    trigrams = getNGrams(data, 3 , m)\n",
    "    for i in range(0,m):\n",
    "        Ngrams.append(unigrams[i] + bigrams[i] + trigrams[i])\n",
    "    df = pd.DataFrame(Ngrams, columns=['Unigrams', 'Absolute Freq', 'Relative Freq', \n",
    "                                       'Bigrams', 'Absolute Freq', 'Relative Freq', \n",
    "                                       'Trigrams', 'Absolute Freq', 'Relative Freq',]) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Top 20 Ngrams in the whole corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords, emojies, and punctuation has been removed from the corpus. \n",
    "#### Ngrams are formed by the lemma of the word. For example: \"go\" includes the frequencies of go, goes, went, gone, etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printNgrams(clean_text(' '.join(df['Text'])), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Top 20 Ngrams in Advertencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = df[df[\"Type\"] == \"advertencia\"]\n",
    "printNgrams(clean_text(' '.join(dfg['Text'])), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Top 20 Ngrams in Seguimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = df[df[\"Type\"] == \"seguimiento\"]\n",
    "printNgrams(clean_text(' '.join(dfg['Text'])), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exporting to html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to html 9_NLP_Word_Tokenization.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

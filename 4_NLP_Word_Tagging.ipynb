{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Installs, Imports and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#!pip install spacy==3.1.1 #restart runtime after this\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet');\n",
    "\n",
    "import string\n",
    "import operator\n",
    "from itertools import islice\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  2. Reading files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Data: 666\n",
      "\n",
      "Columns :  ['Filename', 'Text', 'Type', 'Year']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/alertas.csv', sep=\"|\")\n",
    "df = df[df[\"Text\"] != \" \"]\n",
    "print(\"Size of Data:\", len(df))\n",
    "print()\n",
    "print(\"Columns : \", list(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopw = [\"'s\", \"s\", \"@\", '*', '’', \"t\", \"gt\", \"http\", \"https\", \"amp\", \"m\", 'i', 'u', 'youtu.be/Sj9uLcw-yl4', \n",
    "        \"'m\", '\\-', '[', ']', '·', 're', '“', '”']\n",
    "#m is the number of top ngrams.\n",
    "def getNPartsOfSpeech(text, m, tag):\n",
    "    h_dict = {}   \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    #lemmatization and filtering allowed tags\n",
    "    filtered_tags = [wordnet_lemmatizer.lemmatize(pt[0], pos=\"v\") for pt in pos_tags if pt[1].startswith(tag)]\n",
    "    filtered_tags = [f for f in filtered_tags if f not in stopw]\n",
    "    \n",
    "    for ft in filtered_tags:    \n",
    "        if ft not in h_dict:\n",
    "            h_dict[ft] = 0\n",
    "        h_dict[ft] += 1\n",
    "        \n",
    "    sorted_dict = sorted(h_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    top_values = list(islice(sorted_dict, m))\n",
    "    return [(r[0], r[1], round(r[1]/len(filtered_tags)*100,2)) for r in top_values]\n",
    "\n",
    "def printNPOS(data, m):\n",
    "    postags = []\n",
    "    \n",
    "    verbs = getNPartsOfSpeech(data, m, 'V')\n",
    "    verbs += [(None, None)] * (m - len(verbs))\n",
    "    \n",
    "    adjs = getNPartsOfSpeech(data, m , 'J')\n",
    "    adjs +=[(None, None)] * (m - len(adjs))\n",
    "    \n",
    "    nouns = getNPartsOfSpeech(data, m , 'N')\n",
    "    nouns +=[(None, None)] * (m - len(nouns))\n",
    "    \n",
    "    for i in range(0,m):\n",
    "        if all(verbs[i]) or all(adj[i]) or all(nouns[i]):\n",
    "            postags.append(verbs[i] + adjs[i] + nouns[i])\n",
    "    df = pd.DataFrame(postags, columns=['Verbs', 'Absolute Freq', 'Relative Freq', \n",
    "                                       'Adjectives', 'Absolute Freq', 'Relative Freq',\n",
    "                                        'Nouns', 'Absolute Freq', 'Relative Freq' ]) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Top 20 Verbs, Adj, and Nouns in the whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Verbs</th>\n",
       "      <th>Absolute Freq</th>\n",
       "      <th>Relative Freq</th>\n",
       "      <th>Adjectives</th>\n",
       "      <th>Absolute Freq</th>\n",
       "      <th>Relative Freq</th>\n",
       "      <th>Nouns</th>\n",
       "      <th>Absolute Freq</th>\n",
       "      <th>Relative Freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y</td>\n",
       "      <td>14311</td>\n",
       "      <td>5.87</td>\n",
       "      <td>los</td>\n",
       "      <td>19216</td>\n",
       "      <td>4.59</td>\n",
       "      <td>y</td>\n",
       "      <td>82741</td>\n",
       "      <td>3.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>el</td>\n",
       "      <td>9997</td>\n",
       "      <td>4.10</td>\n",
       "      <td>un</td>\n",
       "      <td>16615</td>\n",
       "      <td>3.97</td>\n",
       "      <td>la</td>\n",
       "      <td>65256</td>\n",
       "      <td>2.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>que</td>\n",
       "      <td>5750</td>\n",
       "      <td>2.36</td>\n",
       "      <td>las</td>\n",
       "      <td>16007</td>\n",
       "      <td>3.83</td>\n",
       "      <td>que</td>\n",
       "      <td>50737</td>\n",
       "      <td>2.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>se</td>\n",
       "      <td>5519</td>\n",
       "      <td>2.27</td>\n",
       "      <td>el</td>\n",
       "      <td>13066</td>\n",
       "      <td>3.12</td>\n",
       "      <td>el</td>\n",
       "      <td>36618</td>\n",
       "      <td>1.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>4772</td>\n",
       "      <td>1.96</td>\n",
       "      <td>que</td>\n",
       "      <td>12648</td>\n",
       "      <td>3.02</td>\n",
       "      <td>con</td>\n",
       "      <td>32216</td>\n",
       "      <td>1.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>del</td>\n",
       "      <td>4740</td>\n",
       "      <td>1.95</td>\n",
       "      <td>una</td>\n",
       "      <td>12548</td>\n",
       "      <td>3.00</td>\n",
       "      <td>del</td>\n",
       "      <td>28036</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>para</td>\n",
       "      <td>4689</td>\n",
       "      <td>1.92</td>\n",
       "      <td>y</td>\n",
       "      <td>9785</td>\n",
       "      <td>2.34</td>\n",
       "      <td>por</td>\n",
       "      <td>24685</td>\n",
       "      <td>1.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>las</td>\n",
       "      <td>4307</td>\n",
       "      <td>1.77</td>\n",
       "      <td>civil</td>\n",
       "      <td>7812</td>\n",
       "      <td>1.87</td>\n",
       "      <td>para</td>\n",
       "      <td>24152</td>\n",
       "      <td>1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>por</td>\n",
       "      <td>3277</td>\n",
       "      <td>1.35</td>\n",
       "      <td>se</td>\n",
       "      <td>7555</td>\n",
       "      <td>1.81</td>\n",
       "      <td>los</td>\n",
       "      <td>22271</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>con</td>\n",
       "      <td>2922</td>\n",
       "      <td>1.20</td>\n",
       "      <td>la</td>\n",
       "      <td>6729</td>\n",
       "      <td>1.61</td>\n",
       "      <td>las</td>\n",
       "      <td>20749</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>al</td>\n",
       "      <td>2774</td>\n",
       "      <td>1.14</td>\n",
       "      <td>por</td>\n",
       "      <td>5571</td>\n",
       "      <td>1.33</td>\n",
       "      <td>El</td>\n",
       "      <td>17216</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>como</td>\n",
       "      <td>2593</td>\n",
       "      <td>1.06</td>\n",
       "      <td>al</td>\n",
       "      <td>4725</td>\n",
       "      <td>1.13</td>\n",
       "      <td>La</td>\n",
       "      <td>17079</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>e</td>\n",
       "      <td>2254</td>\n",
       "      <td>0.93</td>\n",
       "      <td>contra</td>\n",
       "      <td>4670</td>\n",
       "      <td>1.12</td>\n",
       "      <td>como</td>\n",
       "      <td>17060</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>los</td>\n",
       "      <td>2211</td>\n",
       "      <td>0.91</td>\n",
       "      <td>grupos</td>\n",
       "      <td>4250</td>\n",
       "      <td>1.02</td>\n",
       "      <td>se</td>\n",
       "      <td>16965</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>armados</td>\n",
       "      <td>2181</td>\n",
       "      <td>0.90</td>\n",
       "      <td>social</td>\n",
       "      <td>4078</td>\n",
       "      <td>0.97</td>\n",
       "      <td>En</td>\n",
       "      <td>10497</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>•</td>\n",
       "      <td>1952</td>\n",
       "      <td>0.80</td>\n",
       "      <td>lo</td>\n",
       "      <td>3687</td>\n",
       "      <td>0.88</td>\n",
       "      <td>al</td>\n",
       "      <td>9533</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>o</td>\n",
       "      <td>1750</td>\n",
       "      <td>0.72</td>\n",
       "      <td>rural</td>\n",
       "      <td>3623</td>\n",
       "      <td>0.87</td>\n",
       "      <td>San</td>\n",
       "      <td>9252</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>la</td>\n",
       "      <td>1567</td>\n",
       "      <td>0.64</td>\n",
       "      <td>cual</td>\n",
       "      <td>3315</td>\n",
       "      <td>0.79</td>\n",
       "      <td>su</td>\n",
       "      <td>9030</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>es</td>\n",
       "      <td>1553</td>\n",
       "      <td>0.64</td>\n",
       "      <td>en</td>\n",
       "      <td>3213</td>\n",
       "      <td>0.77</td>\n",
       "      <td>ilegales</td>\n",
       "      <td>8032</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>entre</td>\n",
       "      <td>1508</td>\n",
       "      <td>0.62</td>\n",
       "      <td>10-32</td>\n",
       "      <td>2590</td>\n",
       "      <td>0.62</td>\n",
       "      <td>ha</td>\n",
       "      <td>7968</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Verbs  Absolute Freq  Relative Freq Adjectives  Absolute Freq  \\\n",
       "0         y          14311           5.87        los          19216   \n",
       "1        el           9997           4.10         un          16615   \n",
       "2       que           5750           2.36        las          16007   \n",
       "3        se           5519           2.27         el          13066   \n",
       "4        en           4772           1.96        que          12648   \n",
       "5       del           4740           1.95        una          12548   \n",
       "6      para           4689           1.92          y           9785   \n",
       "7       las           4307           1.77      civil           7812   \n",
       "8       por           3277           1.35         se           7555   \n",
       "9       con           2922           1.20         la           6729   \n",
       "10       al           2774           1.14        por           5571   \n",
       "11     como           2593           1.06         al           4725   \n",
       "12        e           2254           0.93     contra           4670   \n",
       "13      los           2211           0.91     grupos           4250   \n",
       "14  armados           2181           0.90     social           4078   \n",
       "15        •           1952           0.80         lo           3687   \n",
       "16        o           1750           0.72      rural           3623   \n",
       "17       la           1567           0.64       cual           3315   \n",
       "18       es           1553           0.64         en           3213   \n",
       "19    entre           1508           0.62      10-32           2590   \n",
       "\n",
       "    Relative Freq     Nouns  Absolute Freq  Relative Freq  \n",
       "0            4.59         y          82741           3.53  \n",
       "1            3.97        la          65256           2.78  \n",
       "2            3.83       que          50737           2.16  \n",
       "3            3.12        el          36618           1.56  \n",
       "4            3.02       con          32216           1.37  \n",
       "5            3.00       del          28036           1.20  \n",
       "6            2.34       por          24685           1.05  \n",
       "7            1.87      para          24152           1.03  \n",
       "8            1.81       los          22271           0.95  \n",
       "9            1.61       las          20749           0.89  \n",
       "10           1.33        El          17216           0.73  \n",
       "11           1.13        La          17079           0.73  \n",
       "12           1.12      como          17060           0.73  \n",
       "13           1.02        se          16965           0.72  \n",
       "14           0.97        En          10497           0.45  \n",
       "15           0.88        al           9533           0.41  \n",
       "16           0.87       San           9252           0.39  \n",
       "17           0.79        su           9030           0.39  \n",
       "18           0.77  ilegales           8032           0.34  \n",
       "19           0.62        ha           7968           0.34  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "printNPOS(' '.join(df['Text']), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Top 20 Verbs, Adj, and Nouns in Advertencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = df[df[\"Type\"] == \"advertencia\"]\n",
    "printNPOS(' '.join(dfg['Text']), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Top 20 Verbs, Adj, and Nouns in Seguimientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = df[df[\"Type\"] == \"seguimiento\"]\n",
    "printNPOS(' '.join(dfg['Text']), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exporting to html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook 8_NLP_Word_Tagging.ipynb to html\n",
      "[NbConvertApp] Writing 586885 bytes to 8_NLP_Word_Tagging.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
